{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossfire Dataset - Hyperparameter Optimization with HPO Module\n",
    "\n",
    "This notebook uses the refactored `hpo.py` module to run multiple HPO experiments with different metrics on the Crossfire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from hpo import run_parallel_optimization, results_to_dataframe\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "print(\"✓ All imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Crossfire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27738, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geo = pd.read_csv('geodf.csv')\n",
    "X = df_geo[['longitude', 'latitude']].values\n",
    "\n",
    "print(f\"✓ Data loaded: {X.shape[0]} points\")\n",
    "print(f\"  Longitude range: [{X[:, 0].min():.4f}, {X[:, 0].max():.4f}]\")\n",
    "print(f\"  Latitude range: [{X[:, 1].min():.4f}, {X[:, 1].max():.4f}]\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_distinct_colors(n_colors):\n",
    "    \"\"\"Generate n distinct colors using multiple colormaps and HSV interpolation.\"\"\"\n",
    "    colors = []\n",
    "    \n",
    "    colormaps = [plt.cm.tab20, plt.cm.Set3, plt.cm.Pastel1, plt.cm.Pastel2, \n",
    "                 plt.cm.Dark2, plt.cm.Accent, plt.cm.Paired]\n",
    "    \n",
    "    for cmap in colormaps:\n",
    "        n_cmap_colors = cmap.N if hasattr(cmap, 'N') else 256\n",
    "        for i in range(n_cmap_colors):\n",
    "            if len(colors) >= n_colors:\n",
    "                break\n",
    "            colors.append(mcolors.to_hex(cmap(i / max(n_cmap_colors - 1, 1))))\n",
    "        if len(colors) >= n_colors:\n",
    "            break\n",
    "    \n",
    "    if len(colors) < n_colors:\n",
    "        remaining = n_colors - len(colors)\n",
    "        for i in range(remaining):\n",
    "            hue = (i * 0.618033988749895) % 1.0\n",
    "            saturation = 0.7 + (i % 3) * 0.1\n",
    "            value = 0.8 + (i % 2) * 0.15\n",
    "            rgb = mcolors.hsv_to_rgb([hue, saturation, value])\n",
    "            colors.append(mcolors.to_hex(rgb))\n",
    "    \n",
    "    return colors[:n_colors]\n",
    "\n",
    "def create_cluster_map(X, labels, algorithm_name, center_lat=-22.9, center_lon=-43.2):\n",
    "    \"\"\"Create a folium map with clustered points.\"\"\"\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels[unique_labels >= 0])\n",
    "    \n",
    "    if n_clusters > 0:\n",
    "        cluster_labels = [label for label in unique_labels if label >= 0]\n",
    "        colors_list = generate_distinct_colors(n_clusters)\n",
    "        color_map = {label: colors_list[i] \n",
    "                    for i, label in enumerate(cluster_labels)}\n",
    "        if -1 in unique_labels or 0 in unique_labels:\n",
    "            noise_label = -1 if -1 in unique_labels else 0\n",
    "            color_map[noise_label] = '#000000'\n",
    "    else:\n",
    "        color_map = {-1: '#000000', 0: '#000000'}\n",
    "    \n",
    "    for i, (lon, lat) in enumerate(X):\n",
    "        label = labels[i]\n",
    "        color = color_map.get(label, '#808080')\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=4,\n",
    "            popup=f\"Cluster: {label}\",\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fillColor=color,\n",
    "            fillOpacity=0.7\n",
    "        ).add_to(m)\n",
    "    \n",
    "    title_html = f'''\n",
    "    <div style=\"position: fixed; \n",
    "                top: 10px; left: 50px; width: 400px; height: 50px; \n",
    "                background-color: white; border:2px solid grey; z-index:9999; \n",
    "                font-size:16px; padding: 10px\">\n",
    "        <b>{algorithm_name}</b><br>\n",
    "        Clusters: {n_clusters} | Total points: {len(X)}\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    return m\n",
    "\n",
    "print(\"✓ Map creation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate center of the data\n",
    "center_lat = X[:, 1].mean()\n",
    "center_lon = X[:, 0].mean()\n",
    "\n",
    "print(f\"Map center: ({center_lat:.4f}, {center_lon:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric Functions\n",
    "\n",
    "We'll use two different metrics:\n",
    "1. **GGDS** (Geo-Granular Density Score) - Uses ConvexHull area\n",
    "2. **Compact GGDS** - Penalizes elongation using cluster diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ggds(points, labels, alpha=1.0, beta=1.0, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Calculates the Geo-Granular Density Score using Scipy.\n",
    "    \n",
    "    Args:\n",
    "        points (np.array): (N, 2) array of coordinates.\n",
    "        labels (np.array): (N,) array of cluster labels. -1 indicates noise.\n",
    "        alpha (float): Weight for cluster count (Granularity).\n",
    "        beta (float): Weight for density (Compactness).\n",
    "        gamma (float): Weight for coverage (Noise penalty).\n",
    "        \n",
    "    Returns:\n",
    "        float: The calculated score.\n",
    "    \"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    if -1 in unique_labels:\n",
    "        unique_labels.remove(-1)\n",
    "    \n",
    "    k = len(unique_labels)\n",
    "    \n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    n_total = len(points)\n",
    "    n_clustered = 0\n",
    "    total_area = 0.0\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        cluster_points = points[labels == label]\n",
    "        n_in_cluster = len(cluster_points)\n",
    "        n_clustered += n_in_cluster\n",
    "        \n",
    "        if n_in_cluster >= 3:\n",
    "            try:\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                total_area += hull.volume \n",
    "            except QhullError:\n",
    "                total_area += 1e-6\n",
    "        else:\n",
    "            total_area += 1e-6\n",
    "\n",
    "    coverage_ratio = n_clustered / n_total\n",
    "    term_granularity = k ** alpha\n",
    "    density = n_clustered / (total_area + 1e-9)\n",
    "    term_density = density ** beta\n",
    "    term_coverage = np.exp(gamma * coverage_ratio)\n",
    "\n",
    "    score = term_granularity * term_density * term_coverage\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def calculate_compact_ggds(points, labels, alpha=1.0, beta=1.0, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Calculates Geo-Granular Score penalizing elongation (Cluster Diameter).\n",
    "    \"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    if -1 in unique_labels: \n",
    "        unique_labels.remove(-1)\n",
    "    \n",
    "    k = len(unique_labels)\n",
    "    if k == 0: \n",
    "        return 0.0\n",
    "\n",
    "    n_total = len(points)\n",
    "    n_clustered = 0\n",
    "    total_span_density = 0.0\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        cluster_points = points[labels == label]\n",
    "        n_in_cluster = len(cluster_points)\n",
    "        n_clustered += n_in_cluster\n",
    "        \n",
    "        diameter = 0.0\n",
    "        \n",
    "        if n_in_cluster >= 3:\n",
    "            try:\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                hull_vertices = cluster_points[hull.vertices]\n",
    "                dists = pdist(hull_vertices, metric='euclidean')\n",
    "                diameter = np.max(dists) if len(dists) > 0 else 0\n",
    "            except QhullError:\n",
    "                dists = pdist(cluster_points)\n",
    "                diameter = np.max(dists) if len(dists) > 0 else 0\n",
    "        elif n_in_cluster == 2:\n",
    "            diameter = np.linalg.norm(cluster_points[0] - cluster_points[1])\n",
    "        else:\n",
    "            diameter = 0.0\n",
    "            \n",
    "        span_density = n_in_cluster / (diameter**2 + 1e-6)\n",
    "        total_span_density += span_density\n",
    "\n",
    "    avg_span_density = total_span_density / k\n",
    "    coverage_ratio = n_clustered / n_total\n",
    "    \n",
    "    term_granularity = k ** alpha\n",
    "    term_compactness = avg_span_density ** beta\n",
    "    term_coverage = np.exp(gamma * coverage_ratio)\n",
    "    \n",
    "    score = term_granularity * term_compactness * term_coverage\n",
    "    \n",
    "    return score\n",
    "\n",
    "print(\"✓ GGDS and Compact GGDS metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter ranges for Crossfire dataset\n",
    "HYPERPARAMETER_RANGES = {\n",
    "    'DTSCAN': {\n",
    "        'MinPts': {'type': 'int', 'range': (2, 25)},\n",
    "        \"area_threshold\": {'type': 'float', 'range': (-50.0, 0)},\n",
    "        'length_threshold': {'type': 'float', 'range': (-50.0, 0)}\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'eps': {'type': 'float', 'range': (0.001, 0.1)},\n",
    "        'min_samples': {'type': 'int', 'range': (3, 30)}\n",
    "    },\n",
    "    'KMeans': {\n",
    "        'n_clusters': {'type': 'int', 'range': (2, 20)},\n",
    "        'n_init': {'type': 'categorical', 'options': [5, 10, 20, 50, 100]}\n",
    "    },\n",
    "    'HDBSCAN': {\n",
    "        'min_cluster_size': {'type': 'int', 'range': (2, 25)},\n",
    "        'alpha': {'type': 'float', 'range': (0.0, 1.0)}\n",
    "    },\n",
    "    'ASCDT': {\n",
    "        'min_cluster_size': {'type': 'int', 'range': (2, 25)},\n",
    "        'beta': {'type': 'float', 'range': (-50.0, 50.0)}\n",
    "    },\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "N_TRIALS = 50\n",
    "ALGORITHMS = ['DTSCAN', 'DBSCAN', 'HDBSCAN', 'KMeans']\n",
    "N_JOBS = -1\n",
    "\n",
    "# Create datasets dictionary (no true labels for crossfire)\n",
    "datasets = {\n",
    "    'Crossfire': (X, np.zeros(len(X)))  # Dummy labels since we use unsupervised metrics\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Algorithms: {ALGORITHMS}\")\n",
    "print(f\"  Trials per task: {N_TRIALS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: HPO with GGDS Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-25 21:15:15] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 37/100\n",
      "[2025-11-25 21:15:15] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 38/100\n",
      "[2025-11-25 21:15:15] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 39/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 9/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 40/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 41/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 42/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 43/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 44/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 45/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 10/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 46/100\n",
      "[2025-11-25 21:15:16] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 47/100\n",
      "[2025-11-25 21:15:17] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 48/100\n",
      "[2025-11-25 21:15:17] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 49/100\n",
      "[2025-11-25 21:15:17] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 11/100\n",
      "[2025-11-25 21:15:17] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 50/100\n",
      "[2025-11-25 21:15:17] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 51/100\n",
      "[2025-11-25 21:15:17] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 12/100\n",
      "[2025-11-25 21:15:17] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 52/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 53/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 54/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 4/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 55/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 56/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 57/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 13/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 58/100\n",
      "[2025-11-25 21:15:18] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 59/100\n",
      "[2025-11-25 21:15:19] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 60/100\n",
      "[2025-11-25 21:15:19] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 14/100[2025-11-25 21:15:19] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 61/100\n",
      "\n",
      "[2025-11-25 21:15:19] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 62/100\n",
      "[2025-11-25 21:15:19] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 63/100\n",
      "[2025-11-25 21:15:19] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 15/100\n",
      "[2025-11-25 21:15:19] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 64/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 65/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 66/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 67/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 68/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 5/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 16/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 69/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 70/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 71/100\n",
      "[2025-11-25 21:15:20] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 17/100\n",
      "[2025-11-25 21:15:21] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 72/100\n",
      "[2025-11-25 21:15:21] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 73/100\n",
      "[2025-11-25 21:15:21] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 18/100\n",
      "[2025-11-25 21:15:21] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 74/100\n",
      "[2025-11-25 21:15:21] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 75/100\n",
      "[2025-11-25 21:15:21] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 76/100\n",
      "[2025-11-25 21:15:22] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 19/100\n",
      "[2025-11-25 21:15:22] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 77/100\n",
      "[2025-11-25 21:15:22] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 78/100\n",
      "[2025-11-25 21:15:22] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 79/100\n",
      "[2025-11-25 21:15:22] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 20/100\n",
      "[2025-11-25 21:15:22] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 80/100\n",
      "[2025-11-25 21:15:22] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 81/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 82/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 21/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 83/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 84/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 85/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 6/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 86/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 22/100\n",
      "[2025-11-25 21:15:23] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 87/100\n",
      "[2025-11-25 21:15:24] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 88/100\n",
      "[2025-11-25 21:15:24] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 89/100\n",
      "[2025-11-25 21:15:24] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 23/100\n",
      "[2025-11-25 21:15:24] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 90/100\n",
      "[2025-11-25 21:15:24] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 91/100\n",
      "[2025-11-25 21:15:24] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 92/100\n",
      "[2025-11-25 21:15:24] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 24/100\n",
      "[2025-11-25 21:15:25] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 93/100\n",
      "[2025-11-25 21:15:25] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 94/100\n",
      "[2025-11-25 21:15:25] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 25/100\n",
      "[2025-11-25 21:15:25] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 95/100\n",
      "[2025-11-25 21:15:25] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 96/100\n",
      "[2025-11-25 21:15:26] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 97/100\n",
      "[2025-11-25 21:15:26] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 26/100\n",
      "[2025-11-25 21:15:26] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 98/100\n",
      "[2025-11-25 21:15:26] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 7/100\n",
      "[2025-11-25 21:15:26] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 99/100\n",
      "[2025-11-25 21:15:26] Dataset: Crossfire, Algorithm: DBSCAN, Trial: 100/100\n",
      "[2025-11-25 21:15:26] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 27/100\n",
      "[2025-11-25 21:15:27] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 28/100\n",
      "[2025-11-25 21:15:27] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 29/100\n",
      "[2025-11-25 21:15:28] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 30/100\n",
      "[2025-11-25 21:15:28] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 8/100\n",
      "[2025-11-25 21:15:28] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 31/100\n",
      "[2025-11-25 21:15:29] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 32/100\n",
      "[2025-11-25 21:15:29] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 33/100\n",
      "[2025-11-25 21:15:30] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 34/100\n",
      "[2025-11-25 21:15:30] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 9/100\n",
      "[2025-11-25 21:15:31] Dataset: Crossfire, Algorithm: KMeans, Trial: 2/100\n",
      "[2025-11-25 21:15:31] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 35/100\n",
      "[2025-11-25 21:15:31] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 36/100\n",
      "[2025-11-25 21:15:32] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 37/100\n",
      "[2025-11-25 21:15:32] Dataset: Crossfire, Algorithm: KMeans, Trial: 3/100\n",
      "[2025-11-25 21:15:32] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 38/100\n",
      "[2025-11-25 21:15:33] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 39/100\n",
      "[2025-11-25 21:15:33] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 10/100\n",
      "[2025-11-25 21:15:33] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 40/100\n",
      "[2025-11-25 21:15:34] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 41/100\n",
      "[2025-11-25 21:15:34] Dataset: Crossfire, Algorithm: KMeans, Trial: 4/100\n",
      "[2025-11-25 21:15:34] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 42/100\n",
      "[2025-11-25 21:15:35] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 43/100\n",
      "[2025-11-25 21:15:36] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 11/100\n",
      "[2025-11-25 21:15:36] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 44/100\n",
      "[2025-11-25 21:15:36] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 45/100\n",
      "[2025-11-25 21:15:37] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 46/100\n",
      "[2025-11-25 21:15:37] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 47/100\n",
      "[2025-11-25 21:15:38] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 48/100\n",
      "[2025-11-25 21:15:38] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 12/100\n",
      "[2025-11-25 21:15:39] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 49/100\n",
      "[2025-11-25 21:15:39] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 50/100\n",
      "[2025-11-25 21:15:40] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 51/100\n",
      "[2025-11-25 21:15:40] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 52/100\n",
      "[2025-11-25 21:15:41] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 13/100\n",
      "[2025-11-25 21:15:41] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 53/100\n",
      "[2025-11-25 21:15:42] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 54/100\n",
      "[2025-11-25 21:15:43] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 55/100\n",
      "[2025-11-25 21:15:43] Dataset: Crossfire, Algorithm: KMeans, Trial: 5/100\n",
      "[2025-11-25 21:15:43] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 56/100\n",
      "[2025-11-25 21:15:43] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 14/100\n",
      "[2025-11-25 21:15:44] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 57/100\n",
      "[2025-11-25 21:15:44] Dataset: Crossfire, Algorithm: KMeans, Trial: 6/100\n",
      "[2025-11-25 21:15:44] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 58/100\n",
      "[2025-11-25 21:15:45] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 59/100\n",
      "[2025-11-25 21:15:45] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 60/100\n",
      "[2025-11-25 21:15:45] Dataset: Crossfire, Algorithm: KMeans, Trial: 7/100\n",
      "[2025-11-25 21:15:46] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 61/100\n",
      "[2025-11-25 21:15:46] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 15/100\n",
      "[2025-11-25 21:15:47] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 62/100\n",
      "[2025-11-25 21:15:47] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 63/100\n",
      "[2025-11-25 21:15:47] Dataset: Crossfire, Algorithm: KMeans, Trial: 8/100\n",
      "[2025-11-25 21:15:48] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 64/100\n",
      "[2025-11-25 21:15:48] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 65/100\n",
      "[2025-11-25 21:15:48] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 16/100\n",
      "[2025-11-25 21:15:48] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 66/100\n",
      "[2025-11-25 21:15:49] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 67/100\n",
      "[2025-11-25 21:15:50] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 68/100\n",
      "[2025-11-25 21:15:50] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 69/100\n",
      "[2025-11-25 21:15:51] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 70/100\n",
      "[2025-11-25 21:15:51] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 17/100\n",
      "[2025-11-25 21:15:51] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 71/100\n",
      "[2025-11-25 21:15:52] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 72/100\n",
      "[2025-11-25 21:15:52] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 73/100\n",
      "[2025-11-25 21:15:53] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 74/100\n",
      "[2025-11-25 21:15:53] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 75/100\n",
      "[2025-11-25 21:15:53] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 18/100\n",
      "[2025-11-25 21:15:54] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 76/100\n",
      "[2025-11-25 21:15:54] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 77/100\n",
      "[2025-11-25 21:15:55] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 78/100\n",
      "[2025-11-25 21:15:55] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 79/100\n",
      "[2025-11-25 21:15:56] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 19/100\n",
      "[2025-11-25 21:15:56] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 80/100\n",
      "[2025-11-25 21:15:56] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 81/100\n",
      "[2025-11-25 21:15:57] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 82/100\n",
      "[2025-11-25 21:15:58] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 83/100\n",
      "[2025-11-25 21:15:58] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 20/100\n",
      "[2025-11-25 21:15:58] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 84/100\n",
      "[2025-11-25 21:15:58] Dataset: Crossfire, Algorithm: KMeans, Trial: 9/100\n",
      "[2025-11-25 21:15:59] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 85/100\n",
      "[2025-11-25 21:15:59] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 86/100\n",
      "[2025-11-25 21:16:00] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 87/100\n",
      "[2025-11-25 21:16:01] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 88/100\n",
      "[2025-11-25 21:16:01] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 21/100\n",
      "[2025-11-25 21:16:01] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 89/100\n",
      "[2025-11-25 21:16:02] Dataset: Crossfire, Algorithm: KMeans, Trial: 10/100\n",
      "[2025-11-25 21:16:02] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 90/100\n",
      "[2025-11-25 21:16:02] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 91/100\n",
      "[2025-11-25 21:16:03] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 92/100\n",
      "[2025-11-25 21:16:03] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 22/100\n",
      "[2025-11-25 21:16:03] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 93/100\n",
      "[2025-11-25 21:16:03] Dataset: Crossfire, Algorithm: KMeans, Trial: 11/100\n",
      "[2025-11-25 21:16:04] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 94/100\n",
      "[2025-11-25 21:16:04] Dataset: Crossfire, Algorithm: KMeans, Trial: 12/100\n",
      "[2025-11-25 21:16:05] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 95/100\n",
      "[2025-11-25 21:16:05] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 96/100\n",
      "[2025-11-25 21:16:06] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 97/100\n",
      "[2025-11-25 21:16:06] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 23/100\n",
      "[2025-11-25 21:16:06] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 98/100\n",
      "[2025-11-25 21:16:07] Dataset: Crossfire, Algorithm: KMeans, Trial: 13/100\n",
      "[2025-11-25 21:16:07] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 99/100\n",
      "[2025-11-25 21:16:07] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 100/100\n",
      "[2025-11-25 21:16:09] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 24/100\n",
      "[2025-11-25 21:16:11] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 25/100\n",
      "[2025-11-25 21:16:13] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 26/100\n",
      "[2025-11-25 21:16:16] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 27/100\n",
      "[2025-11-25 21:16:18] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 28/100\n",
      "[2025-11-25 21:16:20] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 29/100\n",
      "[2025-11-25 21:16:22] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 30/100\n",
      "[2025-11-25 21:16:23] Dataset: Crossfire, Algorithm: KMeans, Trial: 14/100\n",
      "[2025-11-25 21:16:24] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 31/100\n",
      "[2025-11-25 21:16:26] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 32/100\n",
      "[2025-11-25 21:16:26] Dataset: Crossfire, Algorithm: KMeans, Trial: 15/100\n",
      "[2025-11-25 21:16:27] Dataset: Crossfire, Algorithm: KMeans, Trial: 16/100\n",
      "[2025-11-25 21:16:28] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 33/100\n",
      "[2025-11-25 21:16:29] Dataset: Crossfire, Algorithm: KMeans, Trial: 17/100\n",
      "[2025-11-25 21:16:31] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 34/100\n",
      "[2025-11-25 21:16:31] Dataset: Crossfire, Algorithm: KMeans, Trial: 18/100\n",
      "[2025-11-25 21:16:32] Dataset: Crossfire, Algorithm: KMeans, Trial: 19/100\n",
      "[2025-11-25 21:16:33] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 35/100\n",
      "[2025-11-25 21:16:36] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 36/100\n",
      "[2025-11-25 21:16:38] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 37/100\n",
      "[2025-11-25 21:16:39] Dataset: Crossfire, Algorithm: KMeans, Trial: 20/100\n",
      "[2025-11-25 21:16:40] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 38/100\n",
      "[2025-11-25 21:16:42] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 39/100\n",
      "[2025-11-25 21:16:42] Dataset: Crossfire, Algorithm: KMeans, Trial: 21/100\n",
      "[2025-11-25 21:16:44] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 40/100\n",
      "[2025-11-25 21:16:47] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 41/100\n",
      "[2025-11-25 21:16:50] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 42/100\n",
      "[2025-11-25 21:16:53] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 43/100\n",
      "[2025-11-25 21:16:55] Dataset: Crossfire, Algorithm: KMeans, Trial: 22/100\n",
      "[2025-11-25 21:16:55] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 44/100\n",
      "[2025-11-25 21:16:57] Dataset: Crossfire, Algorithm: KMeans, Trial: 23/100\n",
      "[2025-11-25 21:16:58] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 45/100\n",
      "[2025-11-25 21:16:58] Dataset: Crossfire, Algorithm: KMeans, Trial: 24/100\n",
      "[2025-11-25 21:17:00] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 46/100\n",
      "[2025-11-25 21:17:00] Dataset: Crossfire, Algorithm: KMeans, Trial: 25/100\n",
      "[2025-11-25 21:17:01] Dataset: Crossfire, Algorithm: KMeans, Trial: 26/100\n",
      "[2025-11-25 21:17:02] Dataset: Crossfire, Algorithm: KMeans, Trial: 27/100\n",
      "[2025-11-25 21:17:03] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 47/100\n",
      "[2025-11-25 21:17:04] Dataset: Crossfire, Algorithm: KMeans, Trial: 28/100\n",
      "[2025-11-25 21:17:05] Dataset: Crossfire, Algorithm: KMeans, Trial: 29/100\n",
      "[2025-11-25 21:17:05] Dataset: Crossfire, Algorithm: KMeans, Trial: 30/100\n",
      "[2025-11-25 21:17:05] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 48/100\n",
      "[2025-11-25 21:17:08] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 49/100\n",
      "[2025-11-25 21:17:10] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 50/100\n",
      "[2025-11-25 21:17:13] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 51/100\n",
      "[2025-11-25 21:17:16] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 52/100\n",
      "[2025-11-25 21:17:19] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 53/100\n",
      "[2025-11-25 21:17:19] Dataset: Crossfire, Algorithm: KMeans, Trial: 31/100\n",
      "[2025-11-25 21:17:21] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 54/100\n",
      "[2025-11-25 21:17:23] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 55/100\n",
      "[2025-11-25 21:17:25] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 56/100\n",
      "[2025-11-25 21:17:27] Dataset: Crossfire, Algorithm: KMeans, Trial: 32/100\n",
      "[2025-11-25 21:17:28] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 57/100\n",
      "[2025-11-25 21:17:30] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 58/100\n",
      "[2025-11-25 21:17:30] Dataset: Crossfire, Algorithm: KMeans, Trial: 33/100\n",
      "[2025-11-25 21:17:32] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 59/100\n",
      "[2025-11-25 21:17:34] Dataset: Crossfire, Algorithm: KMeans, Trial: 34/100\n",
      "[2025-11-25 21:17:35] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 60/100\n",
      "[2025-11-25 21:17:37] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 61/100\n",
      "[2025-11-25 21:17:37] Dataset: Crossfire, Algorithm: KMeans, Trial: 35/100\n",
      "[2025-11-25 21:17:39] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 62/100\n",
      "[2025-11-25 21:17:40] Dataset: Crossfire, Algorithm: KMeans, Trial: 36/100\n",
      "[2025-11-25 21:17:42] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 63/100\n",
      "[2025-11-25 21:17:44] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 64/100\n",
      "[2025-11-25 21:17:45] Dataset: Crossfire, Algorithm: KMeans, Trial: 37/100\n",
      "[2025-11-25 21:17:45] Dataset: Crossfire, Algorithm: KMeans, Trial: 38/100\n",
      "[2025-11-25 21:17:46] Dataset: Crossfire, Algorithm: KMeans, Trial: 39/100\n",
      "[2025-11-25 21:17:47] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 65/100\n",
      "[2025-11-25 21:17:47] Dataset: Crossfire, Algorithm: KMeans, Trial: 40/100\n",
      "[2025-11-25 21:17:48] Dataset: Crossfire, Algorithm: KMeans, Trial: 41/100\n",
      "[2025-11-25 21:17:49] Dataset: Crossfire, Algorithm: KMeans, Trial: 42/100\n",
      "[2025-11-25 21:17:49] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 66/100\n",
      "[2025-11-25 21:17:51] Dataset: Crossfire, Algorithm: KMeans, Trial: 43/100\n",
      "[2025-11-25 21:17:51] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 67/100\n",
      "[2025-11-25 21:17:53] Dataset: Crossfire, Algorithm: KMeans, Trial: 44/100\n",
      "[2025-11-25 21:17:54] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 68/100\n",
      "[2025-11-25 21:17:55] Dataset: Crossfire, Algorithm: KMeans, Trial: 45/100\n",
      "[2025-11-25 21:17:56] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 69/100\n",
      "[2025-11-25 21:17:57] Dataset: Crossfire, Algorithm: KMeans, Trial: 46/100\n",
      "[2025-11-25 21:17:58] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 70/100\n",
      "[2025-11-25 21:17:58] Dataset: Crossfire, Algorithm: KMeans, Trial: 47/100\n",
      "[2025-11-25 21:18:00] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 71/100\n",
      "[2025-11-25 21:18:03] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 72/100\n",
      "[2025-11-25 21:18:05] Dataset: Crossfire, Algorithm: KMeans, Trial: 48/100\n",
      "[2025-11-25 21:18:05] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 73/100\n",
      "[2025-11-25 21:18:06] Dataset: Crossfire, Algorithm: KMeans, Trial: 49/100\n",
      "[2025-11-25 21:18:07] Dataset: Crossfire, Algorithm: KMeans, Trial: 50/100\n",
      "[2025-11-25 21:18:08] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 74/100\n",
      "[2025-11-25 21:18:09] Dataset: Crossfire, Algorithm: KMeans, Trial: 51/100\n",
      "[2025-11-25 21:18:10] Dataset: Crossfire, Algorithm: KMeans, Trial: 52/100\n",
      "[2025-11-25 21:18:10] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 75/100\n",
      "[2025-11-25 21:18:11] Dataset: Crossfire, Algorithm: KMeans, Trial: 53/100\n",
      "[2025-11-25 21:18:12] Dataset: Crossfire, Algorithm: KMeans, Trial: 54/100\n",
      "[2025-11-25 21:18:12] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 76/100\n",
      "[2025-11-25 21:18:13] Dataset: Crossfire, Algorithm: KMeans, Trial: 55/100\n",
      "[2025-11-25 21:18:14] Dataset: Crossfire, Algorithm: KMeans, Trial: 56/100\n",
      "[2025-11-25 21:18:14] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 77/100\n",
      "[2025-11-25 21:18:15] Dataset: Crossfire, Algorithm: KMeans, Trial: 57/100\n",
      "[2025-11-25 21:18:16] Dataset: Crossfire, Algorithm: KMeans, Trial: 58/100\n",
      "[2025-11-25 21:18:17] Dataset: Crossfire, Algorithm: KMeans, Trial: 59/100\n",
      "[2025-11-25 21:18:17] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 78/100\n",
      "[2025-11-25 21:18:18] Dataset: Crossfire, Algorithm: KMeans, Trial: 60/100\n",
      "[2025-11-25 21:18:19] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 79/100\n",
      "[2025-11-25 21:18:19] Dataset: Crossfire, Algorithm: KMeans, Trial: 61/100\n",
      "[2025-11-25 21:18:20] Dataset: Crossfire, Algorithm: KMeans, Trial: 62/100\n",
      "[2025-11-25 21:18:20] Dataset: Crossfire, Algorithm: KMeans, Trial: 63/100\n",
      "[2025-11-25 21:18:21] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 80/100\n",
      "[2025-11-25 21:18:22] Dataset: Crossfire, Algorithm: KMeans, Trial: 64/100\n",
      "[2025-11-25 21:18:23] Dataset: Crossfire, Algorithm: KMeans, Trial: 65/100\n",
      "[2025-11-25 21:18:23] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 81/100\n",
      "[2025-11-25 21:18:26] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 82/100\n",
      "[2025-11-25 21:18:29] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 83/100\n",
      "[2025-11-25 21:18:32] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 84/100\n",
      "[2025-11-25 21:18:35] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 85/100\n",
      "[2025-11-25 21:18:37] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 86/100\n",
      "[2025-11-25 21:18:38] Dataset: Crossfire, Algorithm: KMeans, Trial: 66/100\n",
      "[2025-11-25 21:18:39] Dataset: Crossfire, Algorithm: KMeans, Trial: 67/100\n",
      "[2025-11-25 21:18:39] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 87/100\n",
      "[2025-11-25 21:18:40] Dataset: Crossfire, Algorithm: KMeans, Trial: 68/100\n",
      "[2025-11-25 21:18:41] Dataset: Crossfire, Algorithm: KMeans, Trial: 69/100\n",
      "[2025-11-25 21:18:42] Dataset: Crossfire, Algorithm: KMeans, Trial: 70/100\n",
      "[2025-11-25 21:18:42] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 88/100\n",
      "[2025-11-25 21:18:42] Dataset: Crossfire, Algorithm: KMeans, Trial: 71/100\n",
      "[2025-11-25 21:18:44] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 89/100\n",
      "[2025-11-25 21:18:47] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 90/100\n",
      "[2025-11-25 21:18:49] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 91/100\n",
      "[2025-11-25 21:18:51] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 92/100\n",
      "[2025-11-25 21:18:51] Dataset: Crossfire, Algorithm: KMeans, Trial: 72/100\n",
      "[2025-11-25 21:18:52] Dataset: Crossfire, Algorithm: KMeans, Trial: 73/100\n",
      "[2025-11-25 21:18:54] Dataset: Crossfire, Algorithm: KMeans, Trial: 74/100\n",
      "[2025-11-25 21:18:54] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 93/100\n",
      "[2025-11-25 21:18:55] Dataset: Crossfire, Algorithm: KMeans, Trial: 75/100\n",
      "[2025-11-25 21:18:57] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 94/100\n",
      "[2025-11-25 21:18:59] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 95/100\n",
      "[2025-11-25 21:19:01] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 96/100\n",
      "[2025-11-25 21:19:03] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 97/100\n",
      "[2025-11-25 21:19:06] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 98/100\n",
      "[2025-11-25 21:19:08] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 99/100\n",
      "[2025-11-25 21:19:10] Dataset: Crossfire, Algorithm: HDBSCAN, Trial: 100/100\n",
      "[2025-11-25 21:19:12] Dataset: Crossfire, Algorithm: KMeans, Trial: 76/100\n",
      "[2025-11-25 21:19:13] Dataset: Crossfire, Algorithm: KMeans, Trial: 77/100\n",
      "[2025-11-25 21:19:13] Dataset: Crossfire, Algorithm: KMeans, Trial: 78/100\n",
      "[2025-11-25 21:19:14] Dataset: Crossfire, Algorithm: KMeans, Trial: 79/100\n",
      "[2025-11-25 21:19:14] Dataset: Crossfire, Algorithm: KMeans, Trial: 80/100\n",
      "[2025-11-25 21:19:15] Dataset: Crossfire, Algorithm: KMeans, Trial: 81/100\n",
      "[2025-11-25 21:19:15] Dataset: Crossfire, Algorithm: KMeans, Trial: 82/100\n",
      "[2025-11-25 21:19:15] Dataset: Crossfire, Algorithm: KMeans, Trial: 83/100\n",
      "[2025-11-25 21:19:16] Dataset: Crossfire, Algorithm: KMeans, Trial: 84/100\n",
      "[2025-11-25 21:19:16] Dataset: Crossfire, Algorithm: KMeans, Trial: 85/100\n",
      "[2025-11-25 21:19:17] Dataset: Crossfire, Algorithm: KMeans, Trial: 86/100\n",
      "[2025-11-25 21:19:21] Dataset: Crossfire, Algorithm: KMeans, Trial: 87/100\n",
      "[2025-11-25 21:19:21] Dataset: Crossfire, Algorithm: KMeans, Trial: 88/100\n",
      "[2025-11-25 21:19:22] Dataset: Crossfire, Algorithm: KMeans, Trial: 89/100\n",
      "[2025-11-25 21:19:30] Dataset: Crossfire, Algorithm: KMeans, Trial: 90/100\n",
      "[2025-11-25 21:19:30] Dataset: Crossfire, Algorithm: KMeans, Trial: 91/100\n",
      "[2025-11-25 21:19:31] Dataset: Crossfire, Algorithm: KMeans, Trial: 92/100\n",
      "[2025-11-25 21:19:31] Dataset: Crossfire, Algorithm: KMeans, Trial: 93/100\n",
      "[2025-11-25 21:19:32] Dataset: Crossfire, Algorithm: KMeans, Trial: 94/100\n",
      "[2025-11-25 21:19:32] Dataset: Crossfire, Algorithm: KMeans, Trial: 95/100\n",
      "[2025-11-25 21:19:33] Dataset: Crossfire, Algorithm: KMeans, Trial: 96/100\n",
      "[2025-11-25 21:19:33] Dataset: Crossfire, Algorithm: KMeans, Trial: 97/100\n",
      "[2025-11-25 21:19:33] Dataset: Crossfire, Algorithm: KMeans, Trial: 98/100\n",
      "[2025-11-25 21:19:35] Dataset: Crossfire, Algorithm: KMeans, Trial: 99/100\n",
      "[2025-11-25 21:19:36] Dataset: Crossfire, Algorithm: KMeans, Trial: 100/100\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION COMPLETE\n",
      "================================================================================\n",
      "Total time: 267.0 seconds (4.4 minutes)\n",
      "Average time per task: 66.7 seconds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run optimization with GGDS metric\n",
    "results_ggds = run_parallel_optimization(\n",
    "    datasets=datasets,\n",
    "    algorithms=ALGORITHMS,\n",
    "    metric_func=calculate_ggds,\n",
    "    hyperparameter_ranges=HYPERPARAMETER_RANGES,\n",
    "    n_trials=100,\n",
    "    experiment_name=\"Crossfire_HPO_GGDS_3\",\n",
    "    n_jobs=N_JOBS,\n",
    "    mlflow_uri=\"file:../mlruns\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GGDS METRIC RESULTS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Score</th>\n",
       "      <th>N Clusters</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>DBSCAN</td>\n",
       "      <td>1.005540e+11</td>\n",
       "      <td>935</td>\n",
       "      <td>1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>{'eps': 0.007473073777533925, 'min_samples': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>7.010207e+10</td>\n",
       "      <td>5050</td>\n",
       "      <td>1</td>\n",
       "      <td>243.3</td>\n",
       "      <td>{'use_scaled': True, 'min_cluster_size': 2, 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>DTSCAN</td>\n",
       "      <td>3.670824e+08</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "      <td>58.5</td>\n",
       "      <td>{'use_scaled': True, 'MinPts': 9, 'area_thresh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>1.916951e+07</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>266.8</td>\n",
       "      <td>{'use_scaled': False, 'n_clusters': 20, 'n_ini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset Algorithm         Score  N Clusters  Expected Time (s)  \\\n",
       "1  Crossfire    DBSCAN  1.005540e+11         935         1     16.9   \n",
       "2  Crossfire   HDBSCAN  7.010207e+10        5050         1    243.3   \n",
       "0  Crossfire    DTSCAN  3.670824e+08         488         1     58.5   \n",
       "3  Crossfire    KMeans  1.916951e+07          20         1    266.8   \n",
       "\n",
       "                                         Best Params  \n",
       "1    {'eps': 0.007473073777533925, 'min_samples': 5}  \n",
       "2  {'use_scaled': True, 'min_cluster_size': 2, 'a...  \n",
       "0  {'use_scaled': True, 'MinPts': 9, 'area_thresh...  \n",
       "3  {'use_scaled': False, 'n_clusters': 20, 'n_ini...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display GGDS results\n",
    "df_ggds = results_to_dataframe(results_ggds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GGDS METRIC RESULTS\")\n",
    "print(\"=\"*80)\n",
    "df_ggds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: HPO with Compact GGDS Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING PARALLEL OPTIMIZATION\n",
      "================================================================================\n",
      "Datasets: ['Crossfire']\n",
      "Algorithms: ['DTSCAN', 'DBSCAN', 'HDBSCAN', 'KMeans']\n",
      "Trials per task: 50\n",
      "Parallelization: Optuna (trials within each study)\n",
      "Trials run in parallel: 8\n",
      "Total tasks: 4\n",
      "================================================================================\n",
      "\n",
      "[2025-11-25 21:19:36] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 1/50\n",
      "[2025-11-25 21:19:41] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 14/50\n",
      "[2025-11-25 21:19:46] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 22/50\n",
      "Step 2: Building graph from triangulation...\n",
      "Step 3: Removing global effects (filtering outlier edges/triangles)...\n",
      "   Building Triangle objects...\n",
      "[2025-11-25 21:19:51] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 31/50\n",
      "[2025-11-25 21:19:56] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 39/50\n",
      "   Computing triangle areas...\n",
      "   Applying z-score normalization to areas (Eq. 2)...\n",
      "   Computing edge lengths...\n",
      "   Computing triangle areas...\n",
      "   Applying z-score normalization to areas (Eq. 2)...\n",
      "   Computing edge lengths...\n",
      "[2025-11-25 21:20:01] Dataset: Crossfire, Algorithm: DTSCAN, Trial: 46/50\n",
      "   Computing triangle areas...\n",
      "   Applying z-score normalization to areas (Eq. 2)...\n",
      "   Computing edge lengths...\n",
      "   Computing triangle areas...\n",
      "   Applying z-score normalization to areas (Eq. 2)...\n",
      "   Computing edge lengths...\n",
      "   Applying z-score normalization to edge lengths (Eq. 3)...\n",
      "   Kept 24824/24828 triangles after area filtering\n",
      "   Final filtered graph has 37210 edges\n",
      "Step 4: Applying density-based clustering...\n"
     ]
    }
   ],
   "source": [
    "# Run optimization with Compact GGDS metric\n",
    "results_compact_ggds = run_parallel_optimization(\n",
    "    datasets=datasets,\n",
    "    algorithms=['DTSCAN', 'DBSCAN', 'HDBSCAN', 'KMeans'],\n",
    "    metric_func=calculate_compact_ggds,\n",
    "    hyperparameter_ranges=HYPERPARAMETER_RANGES,\n",
    "    n_trials=N_TRIALS,\n",
    "    experiment_name=\"Crossfire_HPO_Compact_GGDS\",\n",
    "    n_jobs=N_JOBS,\n",
    "    mlflow_uri=\"file:../mlruns\",\n",
    "    verbose=True,\n",
    "    use_optuna_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Score</th>\n",
       "      <th>N Clusters</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>5.550264e+10</td>\n",
       "      <td>5047</td>\n",
       "      <td>1</td>\n",
       "      <td>99.7</td>\n",
       "      <td>{'use_scaled': False, 'min_cluster_size': 2, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>DBSCAN</td>\n",
       "      <td>1.314342e+10</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>{'eps': 0.0024279799218272424, 'min_samples': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>DTSCAN</td>\n",
       "      <td>5.577617e+08</td>\n",
       "      <td>478</td>\n",
       "      <td>1</td>\n",
       "      <td>30.2</td>\n",
       "      <td>{'use_scaled': False, 'MinPts': 8, 'area_thres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crossfire</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>1.567004e+07</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>{'use_scaled': False, 'n_clusters': 20, 'n_ini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset Algorithm         Score  N Clusters  Expected Time (s)  \\\n",
       "2  Crossfire   HDBSCAN  5.550264e+10        5047         1     99.7   \n",
       "1  Crossfire    DBSCAN  1.314342e+10         176         1      1.8   \n",
       "0  Crossfire    DTSCAN  5.577617e+08         478         1     30.2   \n",
       "3  Crossfire    KMeans  1.567004e+07          20         1      7.3   \n",
       "\n",
       "                                         Best Params  \n",
       "2  {'use_scaled': False, 'min_cluster_size': 2, '...  \n",
       "1  {'eps': 0.0024279799218272424, 'min_samples': 20}  \n",
       "0  {'use_scaled': False, 'MinPts': 8, 'area_thres...  \n",
       "3  {'use_scaled': False, 'n_clusters': 20, 'n_ini...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Compact GGDS results\n",
    "df_compact_ggds = results_to_dataframe(results_compact_ggds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPACT GGDS METRIC RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_compact_ggds.to_string(index=False))\n",
    "\n",
    "df_compact_ggds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Maps with Best Parameters\n",
    "\n",
    "### GGDS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN, KMeans, HDBSCAN\n",
    "from dtscan import DTSCAN\n",
    "from ascdt import ASCDT\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def generate_map_from_result(X, algorithm, params, center_lat, center_lon, return_labels=False):\n",
    "    \"\"\"Generate a map from algorithm and parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    return_labels : bool\n",
    "        If True, also return the labels array\n",
    "    \"\"\"\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    if algorithm == 'DTSCAN':\n",
    "        model = DTSCAN(**params)\n",
    "        f = io.StringIO()\n",
    "        with redirect_stdout(f):\n",
    "            labels = model.fit_predict(X)\n",
    "    elif algorithm == 'DBSCAN':\n",
    "        model = DBSCAN(**params)\n",
    "        labels = model.fit_predict(X_scaled)\n",
    "    elif algorithm == 'KMeans':\n",
    "        model = KMeans(**params, random_state=42)\n",
    "        labels = model.fit_predict(X_scaled)\n",
    "    elif algorithm == 'HDBSCAN':\n",
    "        model = HDBSCAN(**params)\n",
    "        labels = model.fit_predict(X_scaled)\n",
    "    elif algorithm == 'ASCDT':\n",
    "        model = ASCDT(**params)\n",
    "        labels = model.fit_predict(X)\n",
    "    else:\n",
    "        return None if not return_labels else (None, None)\n",
    "    \n",
    "    map_obj = create_cluster_map(X, labels, algorithm, center_lat, center_lon)\n",
    "    return (map_obj, labels) if return_labels else map_obj\n",
    "\n",
    "# Generate maps for GGDS results and store labels for later use\n",
    "print(\"Generating GGDS maps...\")\n",
    "ggds_maps = {}\n",
    "ggds_labels = {}  # Store labels for reuse\n",
    "for algo in ALGORITHMS:\n",
    "    if algo in results_ggds['Crossfire']:\n",
    "        result = results_ggds['Crossfire'][algo]\n",
    "        if result['best_params'].get('use_scaled', None) is not None: del result['best_params']['use_scaled']\n",
    "        map_obj, labels = generate_map_from_result(X, algo, result['best_params'], center_lat, center_lon, return_labels=True)\n",
    "        if map_obj:\n",
    "            ggds_maps[algo] = map_obj\n",
    "            ggds_labels[algo] = labels  # Store labels for convex hull visualization\n",
    "            print(f\"  ✓ {algo}: Score={result['best_score']:.4f}, Clusters={result['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for algo, map_obj in ggds_maps.items():\\n    result = results_ggds[\\'Crossfire\\'][algo]\\n    print(f\"\\n{\\'=\\'*60}\")\\n    print(f\"{algo} - GGDS Metric\")\\n    print(f\"{\\'=\\'*60}\")\\n    print(f\"Score: {result[\\'best_score\\']:.4f}\")\\n    print(f\"Clusters: {result[\\'n_clusters\\']}\")\\n    print(f\"Params: {result[\\'best_params\\']}\")\\n    display(map_obj)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display GGDS maps\n",
    "\"\"\"for algo, map_obj in ggds_maps.items():\n",
    "    result = results_ggds['Crossfire'][algo]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{algo} - GGDS Metric\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Score: {result['best_score']:.4f}\")\n",
    "    print(f\"Clusters: {result['n_clusters']}\")\n",
    "    print(f\"Params: {result['best_params']}\")\n",
    "    display(map_obj)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compact GGDS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate maps for Compact GGDS results and store labels for later use\n",
    "print(\"Generating Compact GGDS maps...\")\n",
    "compact_ggds_maps = {}\n",
    "compact_ggds_labels = {}  # Store labels for reuse\n",
    "for algo in ALGORITHMS:\n",
    "    if algo in results_compact_ggds['Crossfire']:\n",
    "        result = results_compact_ggds['Crossfire'][algo]\n",
    "        if result['best_params'].get('use_scaled', None) is not None: del result['best_params']['use_scaled']\n",
    "        map_obj, labels = generate_map_from_result(X, algo, result['best_params'], center_lat, center_lon, return_labels=True)\n",
    "        if map_obj:\n",
    "            compact_ggds_maps[algo] = map_obj\n",
    "            compact_ggds_labels[algo] = labels  # Store labels for convex hull visualization\n",
    "            print(f\"  ✓ {algo}: Score={result['best_score']:.4f}, Clusters={result['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for algo, map_obj in compact_ggds_maps.items():\\n    result = results_compact_ggds[\\'Crossfire\\'][algo]\\n    print(f\"\\n{\\'=\\'*60}\")\\n    print(f\"{algo} - Compact GGDS Metric\")\\n    print(f\"{\\'=\\'*60}\")\\n    print(f\"Score: {result[\\'best_score\\']:.4f}\")\\n    print(f\"Clusters: {result[\\'n_clusters\\']}\")\\n    print(f\"Params: {result[\\'best_params\\']}\")\\n    display(map_obj)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Compact GGDS maps\n",
    "\"\"\"for algo, map_obj in compact_ggds_maps.items():\n",
    "    result = results_compact_ggds['Crossfire'][algo]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{algo} - Compact GGDS Metric\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Score: {result['best_score']:.4f}\")\n",
    "    print(f\"Clusters: {result['n_clusters']}\")\n",
    "    print(f\"Params: {result['best_params']}\")\n",
    "    display(map_obj)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>GGDS_Score</th>\n",
       "      <th>GGDS_Clusters</th>\n",
       "      <th>Compact_GGDS_Score</th>\n",
       "      <th>Compact_GGDS_Clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DTSCAN</td>\n",
       "      <td>367082420.2760</td>\n",
       "      <td>488</td>\n",
       "      <td>557761689.5877</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DBSCAN</td>\n",
       "      <td>100553965385.7932</td>\n",
       "      <td>935</td>\n",
       "      <td>13143421914.9834</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>70102067967.8418</td>\n",
       "      <td>5050</td>\n",
       "      <td>55502637491.9041</td>\n",
       "      <td>5047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KMeans</td>\n",
       "      <td>19169514.6802</td>\n",
       "      <td>20</td>\n",
       "      <td>15670042.7755</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm         GGDS_Score  GGDS_Clusters Compact_GGDS_Score  \\\n",
       "0    DTSCAN     367082420.2760            488     557761689.5877   \n",
       "1    DBSCAN  100553965385.7932            935   13143421914.9834   \n",
       "2   HDBSCAN   70102067967.8418           5050   55502637491.9041   \n",
       "3    KMeans      19169514.6802             20      15670042.7755   \n",
       "\n",
       "   Compact_GGDS_Clusters  \n",
       "0                    478  \n",
       "1                    176  \n",
       "2                   5047  \n",
       "3                     20  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for algo in ALGORITHMS:\n",
    "    row = {'Algorithm': algo}\n",
    "    \n",
    "    if algo in results_ggds['Crossfire']:\n",
    "        ggds_result = results_ggds['Crossfire'][algo]\n",
    "        row['GGDS_Score'] = f\"{ggds_result['best_score']:.4f}\"\n",
    "        row['GGDS_Clusters'] = ggds_result['n_clusters']\n",
    "    else:\n",
    "        row['GGDS_Score'] = 'N/A'\n",
    "        row['GGDS_Clusters'] = 'N/A'\n",
    "    \n",
    "    if algo in results_compact_ggds['Crossfire']:\n",
    "        compact_result = results_compact_ggds['Crossfire'][algo]\n",
    "        row['Compact_GGDS_Score'] = f\"{compact_result['best_score']:.4f}\"\n",
    "        row['Compact_GGDS_Clusters'] = compact_result['n_clusters']\n",
    "    else:\n",
    "        row['Compact_GGDS_Score'] = 'N/A'\n",
    "        row['Compact_GGDS_Clusters'] = 'N/A'\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests_ggds_sc = comparison_df[['Algorithm', 'GGDS_Score', 'GGDS_Clusters']]\n",
    "bests_ggds_sc.to_csv('best_in_rio.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results in MLflow\n",
    "\n",
    "To view all experiment results in MLflow UI:\n",
    "\n",
    "```bash\n",
    "mlflow ui --backend-store-uri file:../mlruns\n",
    "```\n",
    "\n",
    "Then open http://localhost:5000 in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_map_with_hulls(X, labels, algorithm_name, center_lat=-22.9, center_lon=-43.2):\n",
    "    \"\"\"\n",
    "    Create a folium map showing clusters as convex hull polygons and outliers as points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Input data points (N, 2) array of coordinates\n",
    "    labels : np.ndarray\n",
    "        Cluster labels (N,) array, -1 indicates noise/outliers\n",
    "    algorithm_name : str\n",
    "        Name of the algorithm\n",
    "    center_lat : float\n",
    "        Latitude for map center\n",
    "    center_lon : float\n",
    "        Longitude for map center\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map : Map object with convex hull polygons and outlier points\n",
    "    \"\"\"\n",
    "    from scipy.spatial import ConvexHull, QhullError\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[-22.9843, -43.2232], zoom_start=14)\n",
    "    \n",
    "    # Get unique labels (excluding noise)\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels[unique_labels >= 0])\n",
    "    \n",
    "    # Generate colors for clusters\n",
    "    if n_clusters > 0:\n",
    "        cluster_labels = [label for label in unique_labels if label >= 0]\n",
    "        colors_list = generate_distinct_colors(n_clusters)\n",
    "        color_map = {label: colors_list[i] \n",
    "                    for i, label in enumerate(cluster_labels)}\n",
    "    else:\n",
    "        color_map = {}\n",
    "    \n",
    "    # Add convex hull polygons for each cluster\n",
    "    for label in unique_labels:\n",
    "        if label < 0:  # Skip noise points for now\n",
    "            continue\n",
    "            \n",
    "        cluster_points = X[labels == label]\n",
    "        \n",
    "        if len(cluster_points) < 3:\n",
    "            # For clusters with < 3 points, just add them as points\n",
    "            for point in cluster_points:\n",
    "                folium.CircleMarker(\n",
    "                    location=[point[1], point[0]],  # lat, lon\n",
    "                    radius=4,\n",
    "                    popup=f\"Cluster: {label}\",\n",
    "                    color=color_map.get(label, '#808080'),\n",
    "                    fill=True,\n",
    "                    fillColor=color_map.get(label, '#808080'),\n",
    "                    fillOpacity=0.7\n",
    "                ).add_to(m)\n",
    "        else:\n",
    "            try:\n",
    "                # Calculate convex hull\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                hull_vertices = cluster_points[hull.vertices]\n",
    "                \n",
    "                # Create polygon coordinates (lat, lon pairs)\n",
    "                polygon_coords = [[point[1], point[0]] for point in hull_vertices]\n",
    "                # Close the polygon\n",
    "                polygon_coords.append(polygon_coords[0])\n",
    "                \n",
    "                # Add polygon to map\n",
    "                folium.Polygon(\n",
    "                    locations=polygon_coords,\n",
    "                    popup=f\"Cluster {label}\",\n",
    "                    color=color_map.get(label, '#808080'),\n",
    "                    fill=True,\n",
    "                    fillColor=color_map.get(label, '#808080'),\n",
    "                    fillOpacity=0.3,\n",
    "                    weight=2\n",
    "                ).add_to(m)\n",
    "            except QhullError:\n",
    "                # Fallback: add points if hull fails\n",
    "                for point in cluster_points:\n",
    "                    folium.CircleMarker(\n",
    "                        location=[point[1], point[0]],\n",
    "                        radius=4,\n",
    "                        popup=f\"Cluster: {label}\",\n",
    "                        color=color_map.get(label, '#808080'),\n",
    "                        fill=True,\n",
    "                        fillColor=color_map.get(label, '#808080'),\n",
    "                        fillOpacity=0.7\n",
    "                    ).add_to(m)\n",
    "    \n",
    "    # Add outlier points (label == -1)\n",
    "    outlier_points = X[labels == -1]\n",
    "    for point in outlier_points:\n",
    "        folium.CircleMarker(\n",
    "            location=[point[1], point[0]],  # lat, lon\n",
    "            radius=3,\n",
    "            popup=\"Outlier\",\n",
    "            color='#000000',\n",
    "            fill=True,\n",
    "            fillColor='#000000',\n",
    "            fillOpacity=0.8\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Add title\n",
    "    n_outliers = len(outlier_points)\n",
    "    title_html = f'''\n",
    "    <div style=\"position: fixed; \n",
    "                top: 10px; left: 50px; width: 400px; height: 60px; \n",
    "                background-color: white; border:2px solid grey; z-index:9999; \n",
    "                font-size:16px; padding: 10px\">\n",
    "        <b>{algorithm_name}</b><br>\n",
    "        Clusters: {n_clusters} | Outliers: {n_outliers} | Total points: {len(X)}\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    return m\n",
    "\n",
    "print(\"✓ Convex hull map function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate maps with convex hulls for clusters and outlier points\n",
    "print(\"Generating convex hull maps for GGDS results...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_hull_maps_from_labels(X, labels_dict):\n",
    "    \"\"\"Generate convex hull maps using stored labels (no model re-execution).\"\"\"\n",
    "    # Calculate center for maps\n",
    "    center_lat = X[:, 1].mean()\n",
    "    center_lon = X[:, 0].mean()\n",
    "    \n",
    "    hull_maps = {}\n",
    "    \n",
    "    for algorithm, labels in labels_dict.items():\n",
    "        print(f\"\\n{algorithm}:\")\n",
    "        \n",
    "        try:\n",
    "            # Create convex hull map using the stored labels\n",
    "            hull_maps[algorithm] = create_cluster_map_with_hulls(\n",
    "                X, labels, f\"{algorithm} (Convex Hulls)\", center_lat, center_lon\n",
    "            )\n",
    "            \n",
    "            n_clusters = len(np.unique(labels[labels != -1]))\n",
    "            n_outliers = np.sum(labels == -1)\n",
    "            print(f\"  ✓ Created hull map: {n_clusters} clusters, {n_outliers} outliers\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to create hull map: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return hull_maps\n",
    "\n",
    "# Generate hull maps for GGDS results using stored labels (no re-execution!)\n",
    "hull_maps_ggds = generate_hull_maps_from_labels(X, ggds_labels)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(hull_maps_ggds)} convex hull maps for GGDS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Display the convex hull maps for GGDS results\n",
    "for algorithm, map_obj in hull_maps_ggds.items():\n",
    "    result = results_ggds['Crossfire'][algorithm]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{algorithm} - Convex Hull Visualization (GGDS)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Score: {result['best_score']:.4f}\")\n",
    "    print(f\"N Clusters: {result['n_clusters']}\")\n",
    "    display(map_obj)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hull maps for Compact GGDS results using stored labels\n",
    "print(\"Generating convex hull maps for Compact GGDS results...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hull_maps_compact_ggds = generate_hull_maps_from_labels(X, compact_ggds_labels)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(hull_maps_compact_ggds)} convex hull maps for Compact GGDS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Display the convex hull maps for Compact GGDS results\n",
    "for algorithm, map_obj in hull_maps_compact_ggds.items():\n",
    "    result = results_compact_ggds['Crossfire'][algorithm]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{algorithm} - Convex Hull Visualization (Compact GGDS)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Score: {result['best_score']:.4f}\")\n",
    "    print(f\"N Clusters: {result['n_clusters']}\")\n",
    "    display(map_obj)\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
